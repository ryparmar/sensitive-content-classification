{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be60c0e-4e19-4870-abdc-fed284d79d05",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1cc06-4c84-446c-b5d3-72e7a5e6e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "\n",
    "from nltk import tokenize\n",
    "from nlpaug.util import Action\n",
    "from random import randrange\n",
    "\n",
    "# Note: tqdm <= v4.8\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "tqdm_pandas(tqdm())\n",
    "\n",
    "# Note: tqdm > v4.8\n",
    "# from tqdm.auto import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "from utils import merge_title_perex_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede4e05-ddb1-48a6-bd7a-3f4e432b2ad9",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11a271-16de-4b37-9538-5821d264597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "SENSITIVE_DATA_FILEPATH = \"data/processed-sensitive-data.csv\"\n",
    "SENSITIVE_DATA_AUGMENTED_FILEPATH = \"data/processed-sensitive-data-augmented.csv\"\n",
    "\n",
    "# Multiple splits can be created. Id of new augmented text is created by suffixing the id of the original text by SPLIT\n",
    "# variable. Due to that, we ca track the original id, which was used for augmentation.\n",
    "SPLIT = 4\n",
    "RANDOM_SEED = 11\n",
    "\n",
    "# Output file\n",
    "AUGMENTED_DATA_SPLIT = f\"data/augmented_{SPLIT}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e080a3-8961-49f9-adc9-7a89d5c58ae0",
   "metadata": {},
   "source": [
    "## Read sensitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad4752-8365-4c4b-9b82-3fddda73e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensitive = pd.read_csv(SENSITIVE_DATA_FILEPATH, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc41959-d87e-43ad-9bc1-cb85761db55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensitive.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacce78a-4cdc-4d39-9ccb-50dab8b0d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_sensitive.iloc[0][\"title\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fcf7e-ac1d-467a-bb87-90637477e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensitive[\"text\"] = df_sensitive.progress_apply(merge_title_perex_body, axis=1)\n",
    "df_sensitive[\"text\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17883223-82e2-4e07-88af-feae539b0d6a",
   "metadata": {},
   "source": [
    "### Split to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29b567-992b-45fb-816a-086bf843ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensitive[\"sentences\"] = df_sensitive[\"text\"].progress_apply(tokenize.sent_tokenize)\n",
    "df_sensitive[\"sentences\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505a8aa-bfdd-4537-a884-56e30be1685f",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05daf55-1cc0-44a5-9fb2-1482dae4fc16",
   "metadata": {},
   "source": [
    "The idea of augmenting the data is that we split the text into sentences and slightly change each of the sentence.\n",
    "\n",
    "Used augmentations:\n",
    "- randomly swap words\n",
    "- delete random word or random sequence of words\n",
    "- replace some words with wordnet synonyms or paraphrase database synonyms\n",
    "- insert or substitute a words using language model (roberta-base used)\n",
    "- insert or substitute a words using word2vec vectors (google news vectors used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884ec3f-21c0-4099-9a49-ee679da5d198",
   "metadata": {},
   "source": [
    "### Define augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9affaa-afad-455e-99f3-fffe2ab36ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTERS = {\n",
    "    \"replace-synon-wordnet\": naw.SynonymAug(aug_src=\"wordnet\"),\n",
    "    \"replace-synon-ppdb\": naw.SynonymAug(aug_src=\"ppdb\", model_path=\"ppdb-2.0-s-all\"),\n",
    "    \"swap-word\": naw.RandomWordAug(action=\"swap\"),\n",
    "    \"delete-word\": naw.RandomWordAug(),\n",
    "    \"delete-seq-of-words\": naw.RandomWordAug(action=\"crop\"),\n",
    "    \"substitue-lm\": naw.ContextualWordEmbsAug(model_path=\"roberta-base\", action=\"substitute\"),\n",
    "    \"insert-lm\": naw.ContextualWordEmbsAug(model_path=\"roberta-base\", action=\"insert\"),\n",
    "    \"substitue-w2v\": naw.WordEmbsAug(model_type=\"fasttext\", model_path=\"crawl-300d-2M.vec\", action=\"substitute\"),\n",
    "    \"insert-w2v\": naw.WordEmbsAug(model_type=\"fasttext\", model_path=\"crawl-300d-2M.vec\", action=\"insert\"),\n",
    "}\n",
    "\n",
    "id_to_key = {i: key for i, (key, val) in enumerate(AUGMENTERS.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad09129-eb3f-44b0-a9a8-594a2157dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943accb-c5c1-4430-8a16-3adc1330e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_augmentation(text):\n",
    "    augmentation = id_to_key[randrange(len(AUGMENTERS))]\n",
    "    try:\n",
    "        return AUGMENTERS[augmentation].augment(text)\n",
    "    except Exception as E:\n",
    "        print(f\"Exception for {augmentation} augmentation, using sentence without augmentation. {E}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483196c6-90d2-47b1-94c7-88384433fe76",
   "metadata": {},
   "source": [
    "### Apply random augmentation on each sentence separately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a58ed-118d-4a30-932a-75d3b2f7e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = df_sensitive[\"sentences\"].explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18454bc6-139d-4594-8877-1eedc5388cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented = df_sentences.progress_apply(apply_random_augmentation)\n",
    "df_augmented = df_augmented.rename(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909910c-9f55-4813-9fe6-3d1c5bb31f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix the id of the original text by SPLIT variable\n",
    "df_augmented.index = df_augmented.index.map(lambda x: f\"{SPLIT}{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d522f-748a-48fc-af8e-31bac95c537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7513e-c75a-4110-b93e-d35e8e1259de",
   "metadata": {},
   "source": [
    "### Group sentences back and save augmentation split into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c0aa7-2664-4b00-8d9f-f46fccd488a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented.groupby(\"id\").transform(lambda x: \" \".join(x)).drop_duplicates().to_csv(AUGMENTED_DATA_SPLIT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
